# 多元线性回归

## 表示一个多元线性回归函数

上次我们得到了一个简单的一元线性回归函数：
$$
h_\theta(x)=\theta_0+\theta_1x
$$
很显然这个函数几乎没什么用，实际上很多时候大部分事物都不是由一个特征值决定的。

这次学习一个强大一点的：多元线性回归。

### 符号表示

1. $n$ 特征的数量
2. $x^{(i)}$ 输入样本的第$i$ 个向量特征
3. $x^{(i)}_{j}$ 输入样本的第$i$ 个向量特征的第$j$ 个特征值

### 多元线性回归函数的形式

我们知道一元线性回归的假设函数为：
$$
h_\theta(x)=\theta_0+\theta_1x
$$
那么很显然，我们可以拓展一下，容易得到多元线性回归函数：
$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\cdots+\theta_nx_n
$$

### 抽取特征向量

得到函数：
$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\cdots+\theta_nx_n
$$
我们尝试将变量用向量的形式表达。

* 令$x_0=1$ 。

* 首先我们可以提取变量x：
	$$
	X
	=
	\begin{bmatrix}
		x_0 \\
		x_1 \\
		x_2 \\
		\vdots \\
		x_n\\
	\end{bmatrix}
	$$

* 也可以提取$\theta$ :
	$$
	\theta
	=
	\begin{bmatrix}
		\theta_0 \\
		\theta_1 \\
		\theta_2 \\
		\vdots \\
		\theta_n\\
	\end{bmatrix}
	$$

### 用向量表示多元线性回归函数

我们令$x_0=1$ ,可得假设函数
$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\cdots+\theta_nx_n
$$
 与
$$
h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\cdots+\theta_nx_n
$$
等价。

从第二条等式提取向量$x,\theta$ ，利用矩阵的乘法，可得：
$$
\theta^{T}X
=
\begin{bmatrix}
\theta_0x_0 & \theta_1x_1 & \theta_2x_2 & \theta_3x_3 & \cdots & \theta_nx_n
\end{bmatrix}
$$
因此，假设函数的矩阵表示方式为：
$$
h_{\theta}(x)=\theta^{T}X
$$

## 利用梯度下降法求解多元线性回归假设函数

### 回顾梯度下降

回想一下，代价函数是：
$$
J(\theta_0,\theta_1,\theta_2,\theta_3,\dots,\theta_n)=\frac{1}{2m}\sum\limits^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2
$$
回顾梯度下降的数学表达：
$$
\theta_j=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1,\theta_2,\theta_3,\dots,\theta_n)
$$
由于我们将要用更简介的向量来表示函数，因此我们可以稍作改写：
$$
J(\theta)=\frac{1}{2m}\sum\limits^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2
$$

$$
\theta_j=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)
$$

### 由线性回归的梯度下降推导多元线性回归的梯度下降

回顾一下，线性回归$\theta$ 的更新规则为
$$
\theta_1=\frac{1}{m}\sum\limits^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})·x^{(i)} \\
\theta_0=\frac{1}{m}\sum\limits^m_{i=1}(h_\theta(x^{(i)})-y^{(i)}) \\
$$
我们可以令$x_0=1$ ,重新改写规则，你会发现本质上他们都是这条式子：
$$
\theta_j=\frac{1}{m}\sum\limits^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})·x_j^{(i)} \space j\in\begin{bmatrix}0,1  \end{bmatrix}
$$


由于次数没有发生变化，该规则在多元线性回归上几乎没有改变：
$$
\theta_j=\frac{1}{m}\sum\limits^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})·x_j^{(i)}
$$

## 特征缩放

有时候，特征值的取值的范围可能很大，可能非常小，这会对梯度下降算法的效率产生影响。

### 范围过大或者过小

此时画出的代价函数的图呈现一个瘦长的椭圆形，这种形状不利于梯度下降算法的迭代。如果呈现这种形状，算法在函数中的迭代路线会变得曲折复杂，会非常影响效率。

### 消耗范围，对特征进行缩放

我们通常认为，较小的范围能提升算法效率，我们希望能将特征缩放成$-1\leqslant x \leqslant 1$ 范围附近。可以不那么精确，可以是$-3\leqslant x \leqslant 3$ ，也可以是$-\frac{1}{3}\leqslant x \leqslant \frac{1}{3}$ ，总之不需要太精确，但是也不能太大或者太小。

缩放的方法有很多，我们可以采用直接将数据处以数据的范围，或者采用标准差的方式：$x_i=\frac{x_i-\mu_1}{S_i}$ 





 

