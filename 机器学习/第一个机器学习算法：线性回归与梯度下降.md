# 第一个机器学习算法：线性回归与梯度下降

## 符号解释

* $x^{(i)}$,$y^{(i)}$：某个训练样本
* $m$：样本总数量
* $h_{\theta}$：假设函数

## Linear regression（线性回归）

### 如何获得一个线性回归模型？

* 将**训练数据**放入**学习算法**，算法通过计算得到一个**假设函数**。
* 将$x$ (需要预测的数据)，通过$h_\theta$ (假设函数)后，得到$y$ (估计值)。

### 线性回归的假设函数(hypothesis)的表现形式

$$
h_\theta(x)=\theta_0+\theta_1x
$$

很显然这是一个一次函数，使用一次函数是为了方便学习。为了简便，我们通常简写成：
$$
h(x)=\theta_0+\theta_1x
$$

### $\theta_0$与$\theta_1$这两个参数代表的意义

学过一次函数的都知道代表的是什么。$\theta_0$在这里代表的是截距，$\theta_1$代表斜率。在这里我们将会不断调整截距和斜率，尽量得到一个合适的假设函数。我们需要尽量减少真实数据和假设函数的输出之间的平方差。

### 平方差函数

* 方差

  * 表达式$\frac{1}{m}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$ 
  * 还记得距离公式吗？$x^2+y^2=d^2$，因为我们是根据训练数据得出的假设函数，所以x的值其实是相同的。
  * 方差越小，说明假设函数的数据与训练数据越贴合，越贴近，假设函数就越准确。

* 平方差函数(**代价函数**)
  $$
  J(\theta_0,\theta_1)=\frac{1}{2m}\sum\limits^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2
  $$
  而我们的目标是：
  $$
  \mathop{minisize}\limits_{\theta_0\theta_1}\frac{1}{2m}\sum\limits^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2
  $$
  就是希望找到一对$\theta_0\theta_1$使得方差函数是最小的。

## Gradient descent 梯度下降

在上面我们明确了我们的目标：
$$
\mathop{minisize}\limits_{\theta_0\theta_1}\frac{1}{2m}\sum\limits^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2
$$
我们需要一种高效的方法，去寻找方差最小时的解。

### 梯度下降的形象描述

想像一下你在一座大山上，在梯度下降算法中我们要做的就是旋转360度，看看我们的周围，并问自己我要在某个方向上用小碎步尽快下山。如果我们站在山坡上的这一点，你看一下周围你会发现最佳的下山方向，现在你在山上的新起点上 ，你再看看周围，然后再一次想想 ，我应该从什么方向迈着小碎步下山? 然后你按照自己的判断又迈出一步 ，往那个方向走了一步，然后重复上面的步骤 ，从这个新的点，你环顾四周，并决定从什么方向将会最快下山 ，然后又迈进了一小步，又是一小步，并依此类推，直到你接近局部最低点的位置。

### 梯度下降的数学表达

梯度下降是一种不断且同时更新的。我们采用一次函数来学习，因此只需要更新两个值：
$$
\theta_j=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)
$$
其中$\alpha$是成长速率，就是每一次更新的步长。

其中要注意的是，$\theta$是先计算出来再赋值。也就是说，所有$\theta$的更新不会因为别的$\theta$先更新了而被影响。

### $\alpha$的大小对梯度下降的影响

* $\alpha$太小，会导致更新迭代速率慢，要很久才能找局部最优解。
* $\alpha$太大，会导致无法靠近代价函数的底部，会导致算法是往上走而不是往下走。

因此，$\alpha$要控制好大小，但是直观点看是宁愿偏小也不要过大。

### 为什么梯度下降找到的是局部最优解而不是全局最优解

* 代价函数不一定是只有一个谷底的，可能有几个谷底。

* 如果只有一个谷底，那么梯度下降找到的一定是全局最优解。

* 而不止一个谷底的时候，我们观察一下表达式：
  $$
  \theta_j=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)
  $$
  当到达某个谷谷底，但该谷底不是最优的。那么此使后面的微积分项代表的是函数的斜率，此时一定为0。那就说明，只要达到谷底，函数就会停止迭代，不会继续去寻找真正的全局最优解。

* 因此我们可以得出一个结论：一开始选的起始点会影响最后解的结果，迭代出来的不一定是全局最优解。

## 两者结合，得到第一个简单的机器学习算法

这里是使用一次函数做例子，如果不是一次函数那推广即可。

### 推导

$$
J(\theta_0,\theta_1)=\frac{1}{2m}\sum\limits^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2 \tag{1}
$$

$$
\theta_j=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)\tag{2}
$$

将(1)代入(2):

$$
\theta_j=\theta_j-\alpha\frac{\partial}{\partial\theta_j}\frac{1}{2m}\sum\limits^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2 \tag{3}
$$

将1和0分别代入$\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)$，可得

$$
j=0:\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)=\frac{1}{m}\sum\limits^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})\tag{4}
$$

$$
j=1:\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)=\frac{1}{m}\sum\limits^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})·x^{(i)}\tag{5}
$$

将(4),(5)代入(2)，得：
$$
\theta_0=\theta_0-\alpha\frac{1}{m}\sum\limits^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})
$$

$$
\theta_1=\theta_1-\alpha\frac{1}{m}\sum\limits^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})·x^{(i)}
$$

至此，我们就得到了两个参数的迭代公式。